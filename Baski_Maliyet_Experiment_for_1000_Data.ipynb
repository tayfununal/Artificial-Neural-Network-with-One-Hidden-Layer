{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Baski_Maliyet_Experiment_for_1000_Data.ipynb",
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyM9WclZedqDEG/z/8Qphm8i",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tayfununal/Artificial-Neural-Network-with-One-Hidden-Layer/blob/main/Baski_Maliyet_Experiment_for_1000_Data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sl2uplWmC3qZ"
      },
      "source": [
        "!pip install playground-data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TMhRGNGADDzn"
      },
      "source": [
        "import numpy as np\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import pandas as pd\r\n",
        "import plygdata as pg\r\n",
        "import json\r\n",
        "import requests"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X0swR7YGDGu5"
      },
      "source": [
        "Datas = json.loads(requests.get(\r\n",
        "    \"https://raw.githubusercontent.com/tayfununal/2-Hidden-Layer-Neural-Networks/master/produced_XOR_Datas.json\").text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6TMKUwR9DH3D"
      },
      "source": [
        "def initialization_parameters(x, y, num_node, random_seeds_one, random_seeds_two):\r\n",
        "    np.random.seed(random_seeds_one)\r\n",
        "    W1 = np.random.randn(num_node * x.shape[0]).reshape(num_node, x.shape[0])\r\n",
        "    b1 = np.zeros((num_node, 1))\r\n",
        "\r\n",
        "    np.random.seed(random_seeds_two)\r\n",
        "    W2 = np.random.randn(y.shape[0], num_node)\r\n",
        "    b2 = np.zeros((y.shape[0], 1))\r\n",
        "\r\n",
        "    assert W1.shape == (num_node, x.shape[0])\r\n",
        "    assert b1.shape == (num_node, 1)\r\n",
        "\r\n",
        "    assert W2.shape == (y.shape[0], num_node)\r\n",
        "    assert b2.shape == (y.shape[0], 1)\r\n",
        "\r\n",
        "    parameters = {'W1': W1,\r\n",
        "                  'b1': b1,\r\n",
        "                  'W2': W2,\r\n",
        "                  'b2': b2}\r\n",
        "    return parameters\r\n",
        "\r\n",
        "def sigmoid(x):\r\n",
        "    return 1 / (1 + np.exp(-x))\r\n",
        "\r\n",
        "\r\n",
        "def relu(x):\r\n",
        "    return np.maximum(0, x)\r\n",
        "\r\n",
        "\r\n",
        "def reluDerivative(x):\r\n",
        "    x[x <= 0] = 0\r\n",
        "    x[x > 0] = 1\r\n",
        "    return x\r\n",
        "\r\n",
        "\r\n",
        "def forward_prop(x, parameters):\r\n",
        "    W1 = parameters['W1']\r\n",
        "    b1 = parameters['b1']\r\n",
        "    W2 = parameters['W2']\r\n",
        "    b2 = parameters['b2']\r\n",
        "\r\n",
        "    Z1 = np.dot(W1, x) + b1\r\n",
        "    A1 = relu(Z1)\r\n",
        "    Z2 = np.dot(W2, A1) + b2\r\n",
        "    A2 = sigmoid(Z2)\r\n",
        "\r\n",
        "    assert (A2.shape == (1, x.shape[1]))\r\n",
        "    cache = {\r\n",
        "        'Z1': Z1,\r\n",
        "        'A1': A1,\r\n",
        "        'Z2': Z2,\r\n",
        "        'A2': A2\r\n",
        "    }\r\n",
        "    return A2, cache\r\n",
        "\r\n",
        "\r\n",
        "def backward_prop(x, y, parameters, baski_neuron, cache, learning_rate=0.1):\r\n",
        "    m = y.shape[1]\r\n",
        "\r\n",
        "    W1 = parameters['W1']\r\n",
        "    b1 = parameters['b1']\r\n",
        "    W2 = parameters['W2']\r\n",
        "    b2 = parameters['b2']\r\n",
        "\r\n",
        "    A1 = cache['A1']\r\n",
        "    A2 = cache['A2']\r\n",
        "\r\n",
        "    # Calculations of backward propagation: dW1, db1, dW2, db2\r\n",
        "    baski_W1 = np.zeros((baski_neuron, 2))\r\n",
        "    baski_W1[baski_neuron - 1, 0] = gama * W1[baski_neuron - 1, 0]\r\n",
        "    baski_W1[baski_neuron - 1, 1] = gama * W1[baski_neuron - 1, 1]\r\n",
        "\r\n",
        "    baski_W2 = np.zeros((1, baski_neuron))\r\n",
        "    baski_W2[0, baski_neuron - 1] = gama * W2[0, baski_neuron - 1]\r\n",
        "\r\n",
        "    baski_b1 = np.zeros((baski_neuron, 1))\r\n",
        "    baski_b1[baski_neuron - 1, 0] = gama * b1[baski_neuron - 1, 0]\r\n",
        "\r\n",
        "    dZ2 = A2 - Y\r\n",
        "    dW2 = (1.0 / m) * (np.dot(dZ2, A1.T) + baski_W2)\r\n",
        "    db2 = (1.0 / m) * np.sum(dZ2, axis=1, keepdims=True)\r\n",
        "    dZ1 = np.multiply(np.dot(W2.T, dZ2), reluDerivative(A1))\r\n",
        "    dW1 = (1.0 / m) * (np.dot(dZ1, X.T) + baski_W1)\r\n",
        "    db1 = (1.0 / m) * (np.sum(dZ1, axis=1, keepdims=True) + baski_b1)\r\n",
        "\r\n",
        "    # Updating parameters\r\n",
        "    W1 = W1 - learning_rate * dW1\r\n",
        "    b1 = b1 - learning_rate * db1\r\n",
        "    W2 = W2 - learning_rate * dW2\r\n",
        "    b2 = b2 - learning_rate * db2\r\n",
        "\r\n",
        "    parameters = {'W1': W1,\r\n",
        "                  'b1': b1,\r\n",
        "                  'W2': W2,\r\n",
        "                  'b2': b2}\r\n",
        "    return parameters\r\n",
        "\r\n",
        "\r\n",
        "def cross_entropy_cost(y, A2, baski_neuron):\r\n",
        "    m = y.shape[1]\r\n",
        "\r\n",
        "    W1 = parameters['W1']\r\n",
        "    b1 = parameters['b1']\r\n",
        "    W2 = parameters['W2']\r\n",
        "\r\n",
        "    cross_entropy = np.multiply(np.log(A2 + 1e-15), Y) + np.multiply((1 - Y), np.log(1 - A2 + 1e-15))\r\n",
        "    cost = (1.0 / m) * (- np.sum(cross_entropy) + (gama / 2.0) * (\r\n",
        "                W1[baski_neuron - 1, 0] ** 2 + W1[baski_neuron - 1, 1] ** 2 + W2[0, baski_neuron - 1] ** 2 + b1[\r\n",
        "            baski_neuron - 1, 0] ** 2))\r\n",
        "\r\n",
        "    # Squeezing to avoid unnecessary dimensions\r\n",
        "    cost = np.squeeze(cost)\r\n",
        "    return cost\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ueVrxAaiDQAt"
      },
      "source": [
        "def nn_model(x, y, parameters, baski_neuron, number_of_iter=1000):\r\n",
        "    A2, cache = forward_prop(x, parameters)\r\n",
        "    cost = cross_entropy_cost(y, A2, baski_neuron)\r\n",
        "\r\n",
        "    for i in range(1, number_of_iter):\r\n",
        "        parameters = backward_prop(x, y, parameters, baski_neuron, cache, learning_rate=0.6)\r\n",
        "        A2, cache = forward_prop(x, parameters)\r\n",
        "        cost = cross_entropy_cost(y, A2, baski_neuron)\r\n",
        "\r\n",
        "    return cost, parameters"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vdi66xcoDUZm"
      },
      "source": [
        "np.random.seed(65468)\r\n",
        "random_seeds = np.zeros((100,2))\r\n",
        "for i in range(0,100):\r\n",
        "  for j in range(0,2):\r\n",
        "    random_seeds[i,j] = np.random.randint(1,100001)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fj5dPBoRDWCr"
      },
      "source": [
        "from collections import Counter\r\n",
        "each_of_data_Result = dict()\r\n",
        "\r\n",
        "gama = 12\r\n",
        "\r\n",
        "for n in range(1,101):  #Toplam 100 tane veri grubu olduğundan 100 kere çalıçmasını sağlamak için 101 e kadar saydırmalıyız.\r\n",
        "  df = pd.DataFrame(Datas[\"{}\".format(n)])\r\n",
        "  X = np.array(df[[0,1]]).T\r\n",
        "  Y = np.array(df[[2]]).T\r\n",
        "  print(n)\r\n",
        "\r\n",
        "  average_aHundred_cost = Counter(dict())\r\n",
        "  for i in range(0,100):             #100 farklı başlangıç noktası\r\n",
        "    average_twenty_cost = dict()\r\n",
        "    for number_of_neuron in range(1,22):\r\n",
        "      parameters = initialization_parameters(X, Y, number_of_neuron, int(random_seeds[i,0]), int(random_seeds[i,1]))\r\n",
        "      cost, parameters = nn_model(X , Y, parameters, baski_neuron = number_of_neuron, number_of_iter = 1000)\r\n",
        "      average_twenty_cost['{}'.format(number_of_neuron)] = cost * (1/100)   #100 tane farklı başlangıç noktası ile modeli eğittiğimiz için 100'e bölüyoruz.\r\n",
        "    average_aHundred_cost += Counter(average_twenty_cost)\r\n",
        "  each_of_data_Result[\"{}\".format(n)] = dict(average_aHundred_cost)\r\n",
        "\r\n",
        "print(each_of_data_Result)\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WMHd3DrBDcYf"
      },
      "source": [
        "f = open(\"result.txt\",\"w+\")\r\n",
        "f.write(str(each_of_data_Result))\r\n",
        "f.close()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}